{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711678ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3321beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, m, n):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, embds):\n",
    "        \n",
    "        q = self.q(embds)\n",
    "        k = self.k(embds)\n",
    "        v = self.v(embds)\n",
    "        \n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.m, dim1=self.n))\n",
    "        sims = sims / torch.tensor(k.size(self.n) **0.5)\n",
    "        _attention = F.softmax(sims, dim=self.n)\n",
    "        attention = torch.matmul(_attention, v)\n",
    "\n",
    "        return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19fc460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1450, -0.2672,  0.0806],\n",
       "        [ 0.1443, -0.2664,  0.0807],\n",
       "        [ 0.1469, -0.2696,  0.0806]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(\n",
    "    [[.2, .3, .5],\n",
    "    [.1, .2, .1],\n",
    "    [.9, .1, .7]]\n",
    ")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "sa = SelfAttention(d_model=3, m=0, n=1)\n",
    "\n",
    "sa(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c226680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.3000, 0.5000],\n",
       "        [0.1000, 0.2000, 0.1000],\n",
       "        [0.9000, 0.1000, 0.7000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e43d1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2975,  0.2710, -0.1188],\n",
       "        [-0.2548, -0.5435,  0.2937],\n",
       "        [-0.1119,  0.3462,  0.0803]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.q.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9195fa1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0707,  0.2109, -0.0520],\n",
       "        [ 0.1601, -0.2250,  0.0837],\n",
       "        [ 0.0285, -0.0421, -0.0023]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.k.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3c33bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5047, -0.3487, -0.1850],\n",
       "        [ 0.1797, -0.0968,  0.0276],\n",
       "        [-0.2150, -0.2490,  0.3442]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.v.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b41a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0729,  0.0643,  0.1045],\n",
       "        [-0.0324, -0.0470,  0.0549],\n",
       "        [ 0.1639,  0.4319, -0.0213]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.q(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11349ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0481, -0.0464,  0.0136],\n",
       "        [ 0.0278, -0.0281,  0.0113],\n",
       "        [-0.0277,  0.1378, -0.0400]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.k(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b05ea566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0051, -0.0027,  0.0067],\n",
       "        [ 0.0014,  0.0010, -0.0078],\n",
       "        [-0.0124, -0.0078,  0.0558]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = torch.matmul(sa.q(t), sa.k(t).transpose(dim0=0, dim1=1))\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ad2fe36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.k(t).size(sa.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bb11061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7321)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(sa.k(t).size(sa.n) **0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c430968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0029, -0.0015,  0.0039],\n",
       "        [ 0.0008,  0.0006, -0.0045],\n",
       "        [-0.0072, -0.0045,  0.0322]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_sims = sims / torch.tensor(sa.k(t).size(sa.n) **0.5) # sqrt of dimension\n",
    "\n",
    "scaled_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b83ec833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3324, 0.3329, 0.3347],\n",
       "        [0.3339, 0.3339, 0.3322],\n",
       "        [0.3286, 0.3295, 0.3419]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attention = F.softmax(scaled_sims, dim=sa.n)\n",
    "_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f280661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1450, -0.2672,  0.0806],\n",
       "        [ 0.1443, -0.2664,  0.0807],\n",
       "        [ 0.1469, -0.2696,  0.0806]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(_attention, sa.v(t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Env",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
