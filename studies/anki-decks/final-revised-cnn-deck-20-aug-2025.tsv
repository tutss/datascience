#separator:tab
#html:true
What is CNN convolution and how does it differ from mathematical convolution?	CNN convolution is actually cross-correlation where a filter slides over the input, computing element-wise products and summing them. True mathematical convolution would flip the kernel, but CNNs skip this step for computational efficiency. This operation extracts local patterns by detecting how well the filter matches each region of the input.
What makes CNN convolution special for image processing?	CNN convolution has three key properties that make it powerful for images. It uses local connectivity so each output only depends on a small input region. It shares the same filter weights across all spatial locations, which dramatically reduces parameters. And it provides translation equivariance, meaning the same pattern gets detected regardless of where it appears in the image.
What is a receptive field in CNNs?	The receptive field is the region of the original input that influences a particular neuron's output. Think of it as the "field of view" for each neuron - it starts small in early layers and grows larger as you go deeper, allowing the network to capture increasingly complex patterns from larger areas.
How does the receptive field grow as you add more layers?	The receptive field expands naturally through depth. Early layers see tiny patches like edges and textures. Middle layers combine these into object parts and shapes. Deep layers can see large regions containing complete objects or scenes. Each additional layer with kernels and strides makes the effective field of view larger.
What is max pooling and when is it useful?	Max pooling takes the maximum value from each local region, keeping only the strongest activation. This preserves the most important features while reducing spatial size. It's particularly useful for detecting edges and distinctive patterns because it maintains the presence of key features even if they shift slightly within the pooling window.
What is average pooling and when should you use it?	Average pooling computes the mean value over each local region, creating a smooth downsampling effect. It preserves background information and creates gentler transitions between features. Average pooling works well in the final layers before classification where you want to aggregate information smoothly rather than preserve sharp features.
What is the vanishing gradient problem in deep CNNs?	The vanishing gradient problem occurs when gradients become exponentially smaller as they propagate backward through many layers. This happens because gradients are computed by multiplying many small values together during backpropagation. When gradients become too small, the early layers barely update their weights and essentially stop learning.
How do ReLU activations help solve vanishing gradients?	ReLU activations help prevent vanishing gradients because they have a simple derivative: either 0 or 1. Unlike sigmoid or tanh functions that saturate and produce very small gradients, ReLU maintains strong gradient flow for positive inputs. This allows gradients to propagate more effectively through deep networks without diminishing as severely.
How do skip connections solve the vanishing gradient problem?	Skip connections provide gradient highways that bypass the regular layer-by-layer path. Instead of gradients having to flow through every transformation, they can take direct shortcuts back to earlier layers. This ensures that even very deep networks maintain strong gradient signals for training all layers effectively.
What does the CNN output size formula tell us?	The output size after convolution depends on how the filter moves across the input. With larger kernels or smaller strides, you get smaller outputs. Padding can prevent this shrinkage. The relationship is: $O = \frac{I + 2P - K}{S} + 1$ where larger inputs give larger outputs, while bigger kernels reduce output size.
How does padding affect what happens at image borders?	Padding adds extra values (usually zeros) around the input edges. Without padding, the output shrinks because the filter can't center on border pixels. Same padding maintains the original size when stride equals 1. Padding prevents losing information at image boundaries and keeps spatial dimensions manageable through the network.
Why is parameter sharing crucial for CNNs?	Parameter sharing means using the same filter weights across all spatial positions. This creates massive efficiency gains - instead of learning separate weights for every possible location, the network learns one set of weights that work everywhere. This also provides translation equivariance, so patterns get recognized regardless of where they appear in the image.
How does parameter sharing compare to fully connected layers?	Fully connected layers would need separate weights for every pixel position, creating millions of parameters even for small images. CNNs with parameter sharing only need one set of filter weights regardless of image size. This dramatic reduction makes it practical to process large images while providing better generalization than position-specific weights.
What's the difference between hand-crafted features and learned features?	Hand-crafted features like SIFT and HOG were designed by experts based on domain knowledge. They work the same way for every task and require understanding of what patterns matter. CNN learned features are discovered automatically from data through training. They adapt to each specific task and can find patterns that humans might not think to look for.
Why was LeNet significant for deep learning?	LeNet proved that CNNs could solve real-world pattern recognition problems. It demonstrated that the core ideas of convolution, pooling, and hierarchical feature learning actually worked for practical applications. This success laid the conceptual foundation that modern deep learning would build upon, showing that neural networks could learn useful spatial hierarchies.
What breakthrough did AlexNet represent?	AlexNet showed that deep learning could dramatically outperform traditional computer vision methods on complex, real-world datasets. It proved that CNNs could scale to challenging problems and introduced key techniques like ReLU activations, dropout regularization, and data augmentation that became standard practice throughout deep learning.
What is VGG's design philosophy?	VGG demonstrated that network depth matters more than having large, complex filters. By stacking many small 3×3 convolutions, you can achieve the same receptive field as larger filters while using fewer parameters and more non-linearities. This "go deep with simple building blocks" approach influenced many subsequent architectures.
What innovation made ResNet possible?	ResNet introduced skip connections that let information flow directly between non-adjacent layers. The key insight was adding the input to the output: $y = F(x) + x$. This simple addition solved the vanishing gradient problem and made it possible to train networks with hundreds of layers while maintaining good gradient flow throughout.
What is Inception's approach to multi-scale features?	Inception modules process the same input with multiple filter sizes in parallel, then combine the results. This captures patterns at different scales simultaneously - some filters detect fine details while others capture broader context. The parallel paths provide richer representations than any single filter size could achieve alone.
What is batch normalization and why does it help training?	Batch normalization standardizes each layer's inputs by subtracting the batch mean and dividing by the batch standard deviation, then applies learnable scaling and shifting. The mathematical form is: $BN(x) = \gamma \frac{x-\mu}{\sigma} + \beta$. This stabilizes training by reducing how much the input distribution changes as earlier layers update.
How does batch normalization improve CNN training?	Batch normalization allows much higher learning rates because it prevents activations from becoming too large or small. It reduces the sensitivity to weight initialization and acts as a form of regularization. Most importantly, it enables training very deep networks by maintaining stable activation distributions throughout the forward and backward passes.
What is translation equivariance in CNNs?	Translation equivariance means that if you shift the input image, the output feature maps shift by the same amount. This property comes naturally from the convolution operation - when a pattern moves in the input, the corresponding activation moves in the feature map. It's like the network maintains a consistent spatial relationship between input and output.
What is translation invariance and how is it achieved?	Translation invariance means the final output doesn't change even if the input shifts. While convolution provides equivariance, pooling operations provide invariance by summarizing local regions. Data augmentation during training also helps the network learn to produce consistent outputs regardless of where objects appear in the image.
What are dilated convolutions?	Dilated convolutions insert gaps between filter elements, effectively enlarging the receptive field without adding parameters. A dilation rate of 2 means there's one empty space between each filter weight. This allows the filter to sample a larger area while maintaining the same computational cost and parameter count as a regular convolution.
Why are dilated convolutions useful for dense prediction tasks?	Dilated convolutions solve the trade-off between context and resolution. Traditional approaches use pooling to get larger receptive fields, but this loses spatial detail. Dilated convolutions provide wide context while maintaining full resolution, making them perfect for tasks like semantic segmentation where you need both global understanding and precise boundaries.
What are 1×1 convolutions?	1×1 convolutions apply a fully connected layer to each spatial position independently. They perform linear combinations across channels without any spatial mixing. Despite seeming trivial, they're powerful tools for changing the number of channels, adding non-linearity, and creating computational bottlenecks in efficient architectures.
Why are 1×1 convolutions called "Network in Network"?	The "Network in Network" name comes from the fact that 1×1 convolutions essentially embed a small neural network at each spatial location. They apply the same fully connected transformation to every pixel position, creating a network within the larger network structure that processes channel information.
What are depthwise separable convolutions?	Depthwise separable convolutions split regular convolution into two steps. First, depthwise convolution applies one filter per input channel independently. Then, pointwise convolution uses 1×1 filters to mix information across channels. This factorization dramatically reduces computation while maintaining similar representational power.
Why are depthwise separable convolutions computationally efficient?	The efficiency comes from avoiding the expensive channel mixing during spatial processing. Regular convolution requires $K \times K \times C_{in} \times C_{out}$ operations. Depthwise separable needs only $K \times K \times C_{in} + C_{in} \times C_{out}$ operations. For typical values, this provides 8-10x speedup with minimal accuracy loss.
What is dropout and how does it prevent overfitting?	Dropout randomly sets some neurons to zero during training, forcing the network to develop redundant representations. Each neuron must be useful on its own rather than relying too heavily on specific other neurons. This prevents the network from memorizing training examples and improves generalization to new data.
What are feature maps in CNNs?	Feature maps are the 3D arrays produced after each convolutional layer, with dimensions for height, width, and channels. Each channel represents a different learned filter's response to the input. You can think of feature maps as the network's internal representation of what patterns it detected at each spatial location.
How do feature maps change through network depth?	Feature maps evolve from simple to complex representations as you go deeper. Early layers detect basic patterns like edges and textures with small receptive fields. Middle layers combine these into object parts and shapes. Deep layers represent complete objects and semantic concepts with large receptive fields that span significant portions of the input.
How does backpropagation work differently in CNNs?	CNN backpropagation must account for parameter sharing across spatial locations. When computing gradients for a shared filter, the algorithm accumulates gradient contributions from every spatial position where that filter was applied. This is mathematically expressed as: $\frac{\partial L}{\partial W} = \sum_{positions} \frac{\partial L}{\partial Y} \cdot X$ where the sum covers all spatial applications.
What is the accuracy versus speed trade-off in CNN design?	Deeper and wider networks generally achieve higher accuracy by learning more complex representations, but they require more computation and memory. This creates a fundamental tension in architecture design - you must choose between maximum performance and practical deployment constraints like inference time, memory usage, and energy consumption.
What is the memory versus performance trade-off?	Larger batch sizes often improve training dynamics and final performance, but they require more GPU memory. Similarly, higher resolution inputs provide more detail for the network to learn from, but dramatically increase memory requirements. Practical training requires balancing these factors based on available hardware resources.
What is semantic segmentation?	Semantic segmentation assigns a class label to every pixel in an image. All pixels belonging to the same object category get the same label, regardless of whether they're from different object instances. It's like creating a detailed map that shows exactly where each type of object appears in the image at pixel-level precision.
What is instance segmentation?	Instance segmentation goes beyond semantic segmentation by distinguishing between different objects of the same class. Each individual object gets its own unique mask, so two separate cars in an image would have different labels even though they're both cars. This requires both detecting objects and precisely outlining their boundaries.
What is transfer learning in CNNs?	Transfer learning uses knowledge from one task to improve performance on another task. You start with a network pre-trained on a large dataset, then adapt it to your specific problem. The intuition is that low-level features like edges and textures are useful across many vision tasks, so you don't need to learn them from scratch.
Why does transfer learning work so well for computer vision?	Transfer learning works because CNNs learn hierarchical features that generalize across tasks. Early layers detect universal patterns like edges and textures that appear in almost any image. Later layers can be fine-tuned to detect task-specific patterns while building on these universal foundations, requiring much less data than training from scratch.
What makes good data augmentation for CNNs?	Effective CNN data augmentation preserves the essential content while varying irrelevant aspects. Geometric transforms like rotation and scaling teach invariance to object pose. Photometric changes like brightness and contrast improve robustness to lighting conditions. The key is choosing augmentations that reflect real-world variations your model will encounter.
How does data augmentation improve generalization?	Data augmentation forces the network to learn robust features rather than memorizing specific training examples. By seeing the same object under many different conditions during training, the network learns to focus on essential characteristics that remain consistent across variations. This prevents overfitting and improves performance on new, unseen data.
What is spatial attention in CNNs?	Spatial attention helps the network focus on important regions of the feature map while downweighting less relevant areas. The attention mechanism learns to produce a weight map that highlights where the network should pay attention, similar to how humans focus on important parts of a scene while ignoring distracting background elements.
What is channel attention in CNNs?	Channel attention determines which feature channels are most important for the current input. Different channels detect different patterns, and channel attention learns to emphasize the most relevant pattern detectors while suppressing less useful ones. This adaptive feature selection improves the network's ability to focus on task-relevant information.
Why are activation functions necessary in CNNs?	Activation functions provide the non-linearity that makes deep networks powerful. Without activation functions, no matter how many layers you stack, the entire network would be equivalent to a single linear transformation. Non-linear activations like ReLU allow networks to learn complex, non-linear relationships between inputs and outputs.
What is multi-scale feature learning?	Multi-scale feature learning captures patterns at different spatial scales within the same network. Some objects might be small and require fine-grained features, while others are large and need broader context. Architectures like Feature Pyramid Networks and Inception modules explicitly process multiple scales to handle this diversity.
What are the core principles of CNN architecture design?	Successful CNN architectures follow key principles that match natural image structure. Locality means processing nearby pixels together first. Hierarchy builds complex patterns from simpler ones. Translation equivariance ensures patterns are detected regardless of position. Parameter sharing provides efficiency and generalization. These principles guide effective architecture choices.
What is gradient flow and why can it become problematic?	Gradient flow describes how learning signals propagate backward from the loss to earlier layers during training. Problems arise when gradients become too small (vanishing) and early layers stop learning, or too large (exploding) causing unstable training. Good gradient flow ensures all layers receive meaningful learning signals throughout training.
What distinguishes classification from detection CNNs?	Classification CNNs output a single label for the entire image and use cross-entropy loss. Detection CNNs must both localize objects with bounding boxes and classify them, requiring specialized loss functions that combine localization error and classification error. Detection architectures include additional components for proposing and refining object locations.
What is model compression and why is it important?	Model compression reduces CNN size and computational requirements for deployment on resource-constrained devices like smartphones. Techniques include pruning unimportant weights, quantizing to lower precision, and knowledge distillation where a small student network learns from a large teacher. This makes powerful models practical for real-world applications.