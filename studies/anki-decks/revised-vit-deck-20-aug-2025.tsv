#separator:tab
#html:true
What is IoU and why is it important for object detection?	IoU (Intersection over Union) measures how well a predicted bounding box overlaps with the ground truth box. It's calculated as the area where boxes overlap divided by the total area they cover together. IoU ranges from 0 (no overlap) to 1 (perfect match). It's fundamental because it provides an objective way to measure detection quality and forms the basis for evaluation metrics like mAP.
What are the four main components of Vision Transformer architecture?	Vision Transformers have four key components that work together to process images. Patch embedding divides images into fixed-size patches and converts them to vectors. Positional encoding adds spatial information since transformers don't naturally understand position. Transformer blocks apply self-attention and feed-forward layers to learn relationships between patches. Finally, a classification head processes the special CLS token to make predictions.
How do Vision Transformers convert images into sequences?	ViTs treat images like sentences by dividing them into non-overlapping patches, typically 16×16 pixels each. Each patch gets flattened into a vector and linearly projected to the model's embedding dimension. For a 224×224 image with 16×16 patches, you get 196 patch "tokens" that the transformer processes as a sequence, just like words in natural language processing.
What is multi-head self-attention in Vision Transformers?	Multi-head self-attention allows different parts of the model to focus on different types of relationships simultaneously. The model splits the embedding into multiple "heads" (typically 8-16), each learning different attention patterns. Some heads might focus on local texture details while others capture global shape information. The results from all heads are combined to create rich, multi-faceted representations.
Why does self-attention have quadratic computational complexity?	Self-attention computes relationships between every pair of patches in the image. If you have N patches, the model must compute N×N attention scores to determine how much each patch should attend to every other patch. This quadratic scaling means doubling the image resolution roughly quadruples the computational cost, making ViTs expensive for high-resolution images.
What is semantic segmentation?	Semantic segmentation assigns a class label to every single pixel in an image. For example, it would label every pixel belonging to a car as "car", every sky pixel as "sky", and every road pixel as "road". It treats all instances of the same class identically - two different cars would get the same "car" label without distinguishing between them.
What is instance segmentation?	Instance segmentation goes beyond semantic segmentation by identifying and separately labeling each individual object instance. While semantic segmentation would label all car pixels as "car", instance segmentation would distinguish between "car #1", "car #2", and "car #3". It combines object detection (finding individual objects) with pixel-level segmentation (precise boundaries).
What is panoptic segmentation?	Panoptic segmentation provides a unified approach that combines semantic and instance segmentation. It ensures every pixel gets exactly one label and distinguishes between "things" (countable objects like cars, people) that get instance IDs and "stuff" (amorphous regions like sky, road) that only get semantic labels. It provides complete scene understanding without overlapping or missing pixels.
What is the difference between pre-normalization and post-normalization in transformers?	Pre-normalization applies layer normalization before the attention and feed-forward blocks, while post-normalization applies it afterward. Pre-norm has become the standard for ViTs because it provides much better training stability, especially for deep networks. It enables better gradient flow and reduces the risk of training instabilities that can occur with the original post-norm design.
What is Swin Transformer's key innovation?	Swin Transformer's main innovation is shifted window attention, which processes images in local windows instead of computing global attention over all patches. This makes computation much more efficient because attention is only computed within small windows rather than between all possible patch pairs, reducing complexity from quadratic to linear in image size.
How does shifted window attention work?	Shifted window attention divides the image into non-overlapping windows and computes attention only within each window. In alternating layers, the windows are shifted by half their size, allowing information to flow between previously separated regions. This creates a hierarchical structure where local patterns are captured efficiently while global understanding builds through the shifting mechanism across layers.
What makes CLIP different from traditional supervised learning?	CLIP learns from image-text pairs found on the internet rather than manually labeled categories. Instead of learning to classify images into fixed categories like "dog" or "car", CLIP learns to understand the relationship between images and natural language descriptions. This allows it to perform zero-shot classification by comparing an image to text descriptions of different categories.
How does CLIP enable zero-shot classification?	CLIP creates a shared embedding space where similar images and text descriptions are placed close together. For zero-shot classification, you provide text prompts like "a photo of a dog" for each possible class. CLIP compares the image embedding to all text embeddings and selects the most similar one. This works without seeing specific examples during training because CLIP learned general image-text relationships.
What are the main training challenges for Vision Transformers?	ViTs face several unique training challenges. They require much larger datasets than CNNs because they have weaker built-in assumptions about images. They can be unstable during training without proper techniques like learning rate warmup and gradient clipping. They also tend to overfit on smaller datasets and need stronger regularization than CNNs to generalize well.
How does data hunger affect Vision Transformer training?	ViTs need significantly more training data than CNNs to achieve good performance because they don't have built-in assumptions about spatial structure. While CNNs benefit from knowing that nearby pixels are related, ViTs must learn these spatial relationships from data. This is why ViTs typically require pre-training on massive datasets before fine-tuning on specific tasks.
What role does patch size play in Vision Transformer performance?	Patch size creates a fundamental trade-off between detail and efficiency. Smaller patches capture finer details but create longer sequences that are computationally expensive. Larger patches are more efficient but lose fine-grained information. The standard 16×16 patch size provides a good balance for most tasks, offering reasonable detail while keeping computational costs manageable.
How do CNN and ViT inductive biases differ?	CNNs have strong inductive biases built into their architecture: they assume nearby pixels are related (locality) and that the same pattern should be detected regardless of position (translation equivariance). ViTs have minimal inductive biases and must learn spatial relationships from data. This makes CNNs more data-efficient but potentially limits their flexibility compared to ViTs.
What is attention distance in Vision Transformers?	Attention distance measures how far apart the patches are that attend to each other. Early ViT layers tend to focus on nearby patches (short attention distance), similar to CNN's local receptive fields. Deeper layers develop longer attention distances, focusing on patches far apart to capture global relationships. This progression from local to global is key to ViT's hierarchical learning.
What data augmentation strategies work best for Vision Transformers?	ViTs need stronger data augmentation than CNNs due to their weaker inductive biases. Effective strategies include basic transforms like random cropping and color changes, plus advanced techniques like MixUp (blending two images and their labels) and CutMix (cutting patches from one image and pasting into another). These aggressive augmentations help ViTs learn robust features without overfitting.
How does convolution differ from self-attention for spatial processing?	Convolution processes images with a strong spatial bias, using fixed kernels that scan locally across the image and build up larger receptive fields gradually. Self-attention has a global view from the start, allowing any patch to attend to any other patch immediately. Convolution imposes spatial structure, while self-attention learns spatial relationships from data, explaining why CNNs work with less data but ViTs can be more flexible.
What is transfer learning and why is it effective in computer vision?	Transfer learning uses knowledge gained from one task to improve performance on another task. It works because visual features learned on large datasets often transfer well - edge detectors, texture patterns, and shape features are useful across many different vision tasks. This allows models to achieve good performance on new tasks with much less training data and time.
How do receptive fields differ between CNNs and Vision Transformers?	CNNs start with small local receptive fields that grow gradually through layers as features are combined hierarchically. ViTs have global receptive fields from the first layer since self-attention can connect any patch to any other patch immediately. However, ViTs often learn to focus locally in early layers and globally in later layers, similar to CNNs but with more flexibility.
Why is positional encoding necessary in Vision Transformers?	Self-attention is inherently position-agnostic - it treats input patches as an unordered set. Without positional information, a ViT couldn't distinguish between a cat's head at the top of an image versus at the bottom. Positional encoding adds spatial coordinates to patch embeddings, enabling the model to understand spatial relationships and object structure.
How do Vision Transformers achieve spatial understanding without explicit spatial structure?	ViTs learn spatial relationships through attention patterns rather than having them built-in like CNNs. The attention mechanism allows patches to learn which other patches are relevant based on content and position. Over many layers, these learned attention patterns effectively encode spatial relationships, object boundaries, and hierarchical structure.
What loss functions are commonly used in computer vision?	Different vision tasks use different loss functions matched to their objectives. Classification uses cross-entropy loss to optimize probability distributions over classes. Object detection combines classification loss with localization loss (like smooth L1) for bounding box regression. Segmentation typically uses pixel-wise cross-entropy or Dice loss for precise boundary prediction.
How do model capacity and generalization relate in vision?	Model capacity refers to how complex patterns a model can learn. High-capacity models can fit complex data but risk overfitting on small datasets. Low-capacity models are more robust but may underfit complex patterns. The key is matching model capacity to data size - ViTs have high capacity and need large datasets, while simpler models work better with limited data.
What is backpropagation and why is it fundamental for deep learning?	Backpropagation computes gradients that show how to adjust each parameter to reduce the loss function. It works backward through the network using the chain rule of calculus, efficiently computing gradients for millions of parameters. This enables deep networks to learn complex features automatically by adjusting weights based on their contribution to prediction errors.
What are the key differences between supervised, self-supervised, and unsupervised learning?	Supervised learning uses labeled examples to learn specific tasks like classification. Self-supervised learning creates its own learning signals from unlabeled data, like predicting masked image patches or learning visual-text correspondences. Unsupervised learning discovers patterns without any supervision. Modern vision increasingly uses self-supervised pre-training followed by supervised fine-tuning.
What is multi-task learning and what are its benefits?	Multi-task learning trains a single model to perform multiple related tasks simultaneously, sharing a common feature extraction backbone with task-specific output heads. This improves efficiency by reusing learned features and often improves generalization because different tasks provide complementary supervision signals that help learn better representations.
How do attention weights provide interpretability in Vision Transformers?	Attention weights show which patches the model focuses on when making decisions, providing a window into the model's reasoning process. Different attention heads often learn to focus on different types of patterns - some might attend to object boundaries while others focus on texture. Visualizing these attention patterns helps understand what the model considers important for its predictions.
What are essential preprocessing steps for computer vision?	Key preprocessing includes normalizing pixel values to a standard range, resizing images to consistent dimensions, and applying data augmentation to improve generalization. For ViTs specifically, stronger augmentation is often needed compared to CNNs. Proper train/validation/test splits are crucial to evaluate generalization accurately.
What are the fundamental trade-offs in computer vision model design?	The main trade-offs form a triangle between accuracy, speed, and memory usage. More accurate models typically require more computation and memory. Optimization strategies include model compression techniques, efficient architectures designed for specific constraints, and hardware-specific optimizations. The best choice depends on whether you prioritize research accuracy or practical deployment constraints.
What is the difference between feature extraction and feature learning?	Classical computer vision used hand-crafted features like SIFT or HOG descriptors, designed by experts based on domain knowledge. Deep learning approaches like CNNs and ViTs automatically learn features from data that are optimized for specific tasks. This automatic feature learning eliminates the need for manual feature engineering and often discovers patterns humans wouldn't think to look for.
How does model ensembling improve performance?	Model ensembling combines predictions from multiple different models to achieve better accuracy and robustness than any single model. Different models may make different types of errors, so combining them can reduce overall error rates. The trade-off is increased computational cost and complexity, as you need to run multiple models instead of just one.
What is the relationship between dataset size, model complexity, and performance?	Larger, more complex models generally need more training data to reach their potential without overfitting. Small datasets favor simpler models or transfer learning approaches. ViTs particularly need large datasets due to their high capacity and minimal inductive biases. The key is matching model complexity to available data to achieve the best generalization performance.
Why is regularization essential for training deep vision models?	Regularization prevents overfitting by constraining model complexity or adding noise during training. Techniques include weight decay, dropout, and data augmentation. ViTs often need stronger regularization than CNNs because of their higher capacity and weaker inductive biases. The goal is finding the right balance between model capacity and generalization ability.
What is knowledge distillation in Vision Transformers?	Knowledge distillation trains a smaller "student" model to mimic a larger "teacher" model by learning from the teacher's outputs rather than just the original labels. For ViTs, this can create more efficient models that maintain much of the performance of larger versions. DeiT (Data-efficient image Transformers) popularized distillation for ViTs using special distillation tokens.
How do scaling laws apply to Vision Transformers?	ViT performance scales predictably with model size and training data. Larger models generally perform better but need exponentially more data to reach their potential. There are optimal model sizes for given compute budgets, and performance improvements follow power-law relationships with scale. Understanding these scaling laws helps choose appropriate model sizes for available resources.
When should you choose Vision Transformers over CNNs for a computer vision project?	Choose ViTs when you have large datasets (millions of images), need to capture long-range spatial relationships, or want to leverage powerful pre-trained models like those trained on internet-scale data. Choose CNNs when you have limited training data, need fast inference on edge devices, require strong spatial inductive biases, or are working with small objects that benefit from hierarchical feature learning. Hybrid approaches often provide the best of both worlds.
What are hybrid CNN-ViT architectures and why are they important?	Hybrid architectures combine CNN feature extraction with transformer processing to get benefits from both approaches. For example, a CNN backbone can extract low-level features efficiently, then a ViT head can model global relationships. This allows models to benefit from CNN's spatial inductive biases and data efficiency while gaining ViT's powerful attention mechanisms and long-range modeling capabilities. Examples include ConViT and many modern detection frameworks.
How do computational requirements affect Vision Transformer deployment in real applications?	ViTs are computationally intensive due to quadratic attention complexity and large model sizes. For real-world deployment, consider factors like inference latency, memory constraints, and energy consumption. Mobile applications may require distilled ViTs or hybrid models. Edge devices might need quantization or pruning. Cloud applications can handle larger models but must consider cost and throughput. The choice between accuracy and efficiency depends on specific deployment constraints and requirements.